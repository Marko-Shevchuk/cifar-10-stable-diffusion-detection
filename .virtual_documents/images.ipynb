import pandas as pd
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.utils import image_dataset_from_directory
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import TensorBoard
import time
import matplotlib.pyplot as plt
from matplotlib import gridspec


NAME = "cifakeCNN{}".format(time.strftime("%Y%m%d-%H%M%S"))

tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))


print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))


# gpus = tf.config.list_physical_devices('GPU')
# if gpus:
#   # Restrict TensorFlow to only allocate 1GB of memory on the first GPU
#   try:
#     tf.config.set_logical_device_configuration(
#         gpus[0],
#         [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])
#     logical_gpus = tf.config.list_logical_devices('GPU')
#     print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
#   except RuntimeError as e:
#     # Virtual devices must be set before GPUs have been initialized
#     print(e)


tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)


ds_train = image_dataset_from_directory(
    '../cifar-10-stable-diffusion-detection/train',
    labels='inferred',
    label_mode='binary',
    image_size=[64,64],
    interpolation='nearest',
    batch_size=32,
    shuffle=True,
    seed=69,
    #validation_split=None,
    subset=None,
    follow_links=False,
    crop_to_aspect_ratio=False,
)


ds_test = image_dataset_from_directory(
    '../cifar-10-stable-diffusion-detection/test',
    labels='inferred',
    label_mode='binary',
    image_size=[64,64],
    interpolation='nearest',
    batch_size=32,
    shuffle=True,
    seed=69,
    #validation_split=None,
    subset=None,
    follow_links=False,
    crop_to_aspect_ratio=False,
)


def convert_to_float(image, label):
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    return image, label

AUTOTUNE = tf.data.experimental.AUTOTUNE



ds_train = (
    ds_train
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)



ds_valid = (
    ds_test
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)


early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=8,
    min_delta=0.001,
    restore_best_weights=True)


from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0008)


from tensorflow.keras.callbacks import ModelCheckpoint
timestamp = time.strftime("%Y%m%d-%H%M%S")
model_checkpoint = ModelCheckpoint(
    f'best/best_model_{timestamp}.h5', monitor='val_loss', save_best_only=True, mode='min'
)


import keras_tuner
from keras_tuner import RandomSearch


def build_model(hp):
    model = Sequential([
        layers.InputLayer(input_shape=[64,  64,  3]),
        layers.experimental.preprocessing.RandomRotation(factor=0.1488228133769420),
        layers.experimental.preprocessing.RandomTranslation(height_factor=0.069, width_factor=0.069),
        preprocessing.RandomFlip(mode='horizontal'),
        layers.BatchNormalization(renorm=True),
        layers.Conv2D(filters=hp.Int('conv_1_filter', min_value=2, max_value=4, step=2),
                      kernel_size=hp.Choice('conv_1_kernel', values=[3,  7]),
                      activation='relu', padding='same'),
    ])
    
    if hp.Boolean('use_maxpool_first'):
        model.add(layers.MaxPool2D())
    
    model.add(layers.BatchNormalization(renorm=True))
    model.add(layers.Conv2D(filters=hp.Int('conv_2_filter', min_value=2, max_value=4, step=2),
                            kernel_size=hp.Choice('conv_2_kernel', values=[3,  7]),
                            activation='relu', padding='same'))
    model.add(layers.MaxPool2D())
    model.add(layers.BatchNormalization(renorm=True))
    model.add(layers.Conv2D(filters=hp.Int('conv_3_filter', min_value=2, max_value=4, step=2),
                            kernel_size=hp.Choice('conv_3_kernel', values=[3,  7]),
                            activation='relu', padding='same'))
    model.add(layers.Conv2D(filters=hp.Int('conv_4_filter', min_value=2, max_value=4, step=2),
                            kernel_size=hp.Choice('conv_4_kernel', values=[3,  7]),
                            activation='relu', padding='same'))
    model.add(layers.MaxPool2D())
    model.add(layers.BatchNormalization(renorm=True))
    model.add(layers.Flatten())
    
    # Add dense layers based on hyperparameters
    for i in range(hp.Int('num_dense_layers', min_value=0, max_value=4)):  # Adjust the range as needed
        model.add(layers.Dense(hp.Int(f'dense_{i+1}_units', min_value=1, max_value=513, step=256),
                     activation='relu'))
        model.add(layers.Dropout(hp.Float(f'dropout_{i+1}_rate', min_value=0.0, max_value=0.5, step=0.1)))
    
    model.add(layers.Dense(1, activation='sigmoid'))
    
    optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=0.001, max_value=0.1, sampling='LOG'),
                                         epsilon=0.01)
    model.compile(optimizer=optimizer,
                  loss='binary_crossentropy',
                  metrics=['binary_accuracy'])
    return model




tuner = RandomSearch(
    build_model,
    objective='val_binary_accuracy',
    max_trials=20,
    executions_per_trial=3,
    directory='projects',
    project_name='cifake')



# Start the hyperparameter search with early stopping
tuner.search(ds_train, epochs=50, validation_data=ds_valid, callbacks=[early_stopping, tensorboard, reduce_lr, model_checkpoint])

# Get the best model
best_model = tuner.get_best_models(num_models=1)[0]


best_model.summary()


tuner.results_summary()


best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]

hyperparameters_str = "_".join(f"{k}={v}__" for k, v in best_hyperparameters.get_config().items())

NAME += "_" + hyperparameters_str


best_model.save(f"best/{NAME}.h5")



